\title{COPPE}
\author{}
\date{\today}

\documentclass[12pt]{article}

\usepackage{amsthm}

\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\begin{document}
\maketitle

% \begin{abstract}
% \end{abstract}

\section{Definition}

A postprocessing layer is a function that:
\begin{itemize}
\item is one of the following:
  \begin{itemize}
  \item a normalization function (e.g. batchnorm)
  \item an activation function (e.g. relu)
  \item dropout function
  \end{itemize}
\item does not change the ``type'' (shape, data type, etc) of the input tensor
\end{itemize}

\begin{definition}[valid nework]
  A generated network $N$ is valid if and only if the following hold:
  \begin{itemize}
    \item The ranks of sequentially connected tensors in the network are not increasing ($\rightarrow$ a dense layer (rank 1) cannot be followed by a conv2d (rank 3) layer).
  \item Let $M$ be an arbitrary subgraph of $N$. $\forall M\subseteq N$ with only postprocessing layers, there only exists one layer per type.
  \item Any connected vertices have matching dimensions ($\rightarrow$ this can be fixed by adding upsampling/downsampling/reshape/concatenate layers).
    \end{itemize}
  \end{definition}

\paragraph{Functions}
  \begin{itemize}
\item recipe: [layers] $\rightarrow$ node
\item layer: [hyperparams] $\rightarrow$ node
\item hyperparam: string $\rightarrow$ some types (int, float, list of ints, strings, etc)

    \end{itemize}
\bibliographystyle{abbrv}
\bibliography{main}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
